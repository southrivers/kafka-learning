
生产者：
ack的机制：
首先ack存在0、1、-1三种情况：
0： 不需要确认sender发出去的消息是否被接受，直接由broker返回响应、
1：需要leader固化数据，之后leader会ack到客户端
-1：leader以及isr节点都需要固化信息，之后broker响应客户端ack进行确认


kafka同步、异步
kafka的同步、异步是用来控制发送节奏的，同步的情况下上一个消息在没有得到确认的情况下不会发送下一个消息，而消息的确认机制就是上面的ack的机制，异步的情况下则是在一个消息没有得到确认的情况下，可以继续发送下一个数据。。




nums.network.thread：网络io
nums.io.thread:
socket.receive.buffer.bytes
socket.send.buffer.bytes



log.dirs
log.retention.hour
nums.recovery.thread.per.data.dir
nums.partition
log.retention.check.interval.ms
log.segment.byte


-- 这俩参数都是在服务端进行配置的，并不是客户端可以设置的
replica.lag.time.max.ms: 用来设置fellower副本所在节点的心跳超时时间
min.insync.replicas：用来设定isr队列里面最少得fellower的数量，结合ack的机制，可以实现数据的可靠传输，但是这种情况下会存在重复发送的问题


事务协调器对应的topic包含了50个分区，consumeroffset也包含了50个分区，针对某一个消息记录具体选择那个partition来作为其目标partition呢?



消费语义：
exactly once：至少一次，这种情况下需设置ack的机制为all，并且写入的partition的isr队列里面至少要有两个以上的节点才可以，之所以出现至少一次的语义（也就是重复写的原因是因为生产在出现没有得到确认的情况下的时候会再次尝试发送，因此broker里面就出现了重复的数据）

有序：客户端如果要保证发送数据的有序性，就需要在客户端配置max.in.flight.per.connection<=5，这样在出现乱序数据的时候，比如12453,12会被持久化，而45则会在内存中等待3的到来，这正是依据三元组里面的sequenceNumber来实现的。

幂等：需要客户端开启幂等（enable.idemptent）
幂等会为每一条发往分区的数据添加额外的信息，包含了pid、partition、sequenceNumber，其中pid是会话级别的信息，sequenceNumber是和pid相关联的，一旦客户端发生了重启再次启动的client就会向broker请求并重新获取一个新的id（注意这里的pid并不是processId，而是通过zk来创建的id信息），而broker会缓存最近的5条数据，并依据这个三元组去重，

事务：
事务可以看作是增强版本的幂等，从表象上来看得话是新增了一个事务id信息，这个事务id就是为了解决pid不唯一的问题，也就是说在broker端存在一个map集合，用来存放tid和pid的信息，当一个客户端重启并连接到服务端的时候会getOrCreate来获取pid信息，至于是get还是create则根据transactionid来判断。并且最好使用带key的消息，这样可以确保消息发送分区的有效性。

事务的实现：每一个broker都存在一个transactionCoordinator对象，并且在broker端存在一个特殊的_transaction_state主题，该主题有50个分区，事务的开始是对tc的选择，其选择的策略是对transactionId的hash取50的余数，得到使用_transaction_state的leader分区所在的节点上的tc来进行事务管理，之所以这样选择是后续的事务会持久化到这个分区，这样在将不同的客户端的事务尽可能的打散的情况下，依然确保了由于故障重启带来的重新分区的不确定性，并且持久化的事务消息都会写入到这个topic对应的分区，在出现tc故障的时候其他的fellow tc会提升为leader tc，而新提升的leader会从topic加载数据并重构tc，从而确保事务可以继续。

具体流程如下：
1、请求tc
2、生成或者创建pid
3、发送消息到topicx
4、提交确认请求到tc
5、tc将该确认请求写入transaction_state中，并且在实现了三副本写的前提下响应客户端（确保了事务的高可靠性，不是由客户端直接提交给topicx，这样的话会存在client挂掉的情况）
6、broker响应客户端的请求
7、tc提交commit消息到topicx（tc是多活的，因此这样做的好处就是提升了tc的可用性）
8、tc收到确认的消息（由幂等来确保数据的ok）
9、tc将确认之后的消息写入transaction_state中


消费者：


1、消费者组的形成：
类似于producer里面的事务的机制，在每一个broker端也包含了groupcoordinator对象，同时还有一个consumer_offset（默认有50个分区）用来存放消费者消费的偏移量，同一个group.id的消费者在启动的时候会首先连接到broker，并由broker根据group.id的hashcode对50取余，比如结果是48，那么就会使用consumer_offset的第48个partition的leaderpartition所在节点的groupcoordinator来作为当前group的协调者。接下来consumer就会和这个coordiantor建立连接（joingroup的操作），这样同一个小组内的consumer就都注册进来了，接下来coordinator就会选择一个consumer作为leader，由该leader制作计划，用来实现分区和消费者的映射关系，也就是分区策略，常见的分区策略有roundrobin、range等，这是在consumer中指定参数：partition.assignment.strategy来设置的，这样consumer的leader就会将指定的计划提交到groupcoordinator，而后由groupcoordinator将该策略分发到各个consumer节点。

期间consumer会和coordinator维护一个心跳机制，一旦超时consumer就会被剔除，因此有两个参数比较重要heartbeat.interval.ms和session.timeout.ms也就是心跳以及超时时间。

2、消费数据的流程
consumer消费数据的时候会创建networkclient用来拉取broker中的数据，对应的参数fetch.min.byte用来指定每批次抓取的数据的最小值（默认1字节）、fetch.max.wait.ms用来指定每次抓取数据的时间间隔（500ms），以上两个fetch条件有一个满足的话就会真正的从broker拉取数据， 另外存在一个参数：fetch.max.byte，这个参数的含义其实代表了一个缓存池的大小，默认为50m，也就是和producer的流程非常类似，接下来consumer会从这个缓存池中获取数据，对应的参数max.poll.records则是用来指定每次获取的数据量，当然拉取数据的时候会依次经理拦截、反序列化的操作，默认批次取出来500条数据

3、消费者订阅topic，指定分区、指定offset进行消费
1、subscribe(topics)：直接订阅主题并使用指定的分区分配策略来实现consumer和partition的分配
2、assign(TopicPartition)
3、seek()指定partition和offset进行消费，其中可以指定offset也可以根据时间来选择offset再进行seek
seek：首先获取assignment信息（topicpartition），然后设置就可以了
time：需要offsetForTime来通过时间获取offset，不过需要事先通过assign获取partition信息

4、offset相关：

消费相关：
auto.offset.reset：earlist、latest、none，用来指定消费数据的时候从哪个地方开始消费，其中none这种情况要求在consumer_offset中存在groupid的信息，如果不存在的话这种方式会抛出异常。

提交相关：
enable.auto.commit.offset


自动提交offset
consumer.commitsync()：这种情况在下次的poll之前会等待上次commit成功，否则不会提交上去
手动提交offset
consumer.commitAsync()：这种情况不会等待commit成功就会进行下次的poll













